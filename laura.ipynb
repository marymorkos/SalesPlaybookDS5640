{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_csv_from_github(file_name):\n",
    " base_url = \"https://raw.githubusercontent.com/marymorkos/SalesPlaybookDS5640/refs/heads/main/\"\n",
    " return pd.read_csv(base_url + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laura\\AppData\\Local\\Temp\\ipykernel_13800\\2943978199.py:4: DtypeWarning: Columns (24,25,28,32,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(base_url + file_name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent Company</th>\n",
       "      <th>CCaaS</th>\n",
       "      <th>Annual Revenue</th>\n",
       "      <th>Target Account</th>\n",
       "      <th>Associated Contact</th>\n",
       "      <th>Number of Form Submissions</th>\n",
       "      <th>Total Agents</th>\n",
       "      <th>Web Technologies</th>\n",
       "      <th>Close Date</th>\n",
       "      <th># of Agents Total</th>\n",
       "      <th>...</th>\n",
       "      <th>BPO Program</th>\n",
       "      <th>Primary Sub-Industry</th>\n",
       "      <th>Number of Sessions</th>\n",
       "      <th>WFM</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Create Date</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Last Modified Date</th>\n",
       "      <th>BPO Program Tier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contact_ef780380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Route 53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>HVAC and plumbing supply</td>\n",
       "      <td>2024-10-30 10:51</td>\n",
       "      <td>Company_4fc73a2a</td>\n",
       "      <td>2025-02-14 13:39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contact_93373ba5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pest Control</td>\n",
       "      <td>2024-09-12 18:30</td>\n",
       "      <td>Company_f3f7e884</td>\n",
       "      <td>2025-02-14 13:39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contact_2e8e0993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Youtube; App Nexus; Google Tag Manager; Facebo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Pest Control</td>\n",
       "      <td>2024-09-03 10:44</td>\n",
       "      <td>Company_89929bed</td>\n",
       "      <td>2025-02-14 13:39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Five9</td>\n",
       "      <td>5.000000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contact_635e44ed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Postmark; Facebook Advertiser; Salesforce; Goo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Pest Control</td>\n",
       "      <td>2024-08-02 11:36</td>\n",
       "      <td>Company_33dbf591</td>\n",
       "      <td>2025-02-14 13:39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Five9</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contact_8d055096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Microsoft Office 365; Google Tag Manager; Reca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Pest Control</td>\n",
       "      <td>2024-08-02 11:36</td>\n",
       "      <td>Company_a3079821</td>\n",
       "      <td>2025-02-14 13:39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parent Company  CCaaS  Annual Revenue Target Account Associated Contact  \\\n",
       "0            NaN    NaN    1.000000e+09            NaN   Contact_ef780380   \n",
       "1            NaN    NaN             NaN            NaN   Contact_93373ba5   \n",
       "2            NaN    NaN    5.000000e+07            NaN   Contact_2e8e0993   \n",
       "3            NaN  Five9    5.000000e+07            NaN   Contact_635e44ed   \n",
       "4            NaN  Five9    1.000000e+08            NaN   Contact_8d055096   \n",
       "\n",
       "   Number of Form Submissions  Total Agents  \\\n",
       "0                         0.0           NaN   \n",
       "1                         NaN           NaN   \n",
       "2                         0.0           NaN   \n",
       "3                         0.0           NaN   \n",
       "4                         0.0           NaN   \n",
       "\n",
       "                                    Web Technologies Close Date  \\\n",
       "0                                           Route 53        NaN   \n",
       "1                                                NaN        NaN   \n",
       "2  Youtube; App Nexus; Google Tag Manager; Facebo...        NaN   \n",
       "3  Postmark; Facebook Advertiser; Salesforce; Goo...        NaN   \n",
       "4  Microsoft Office 365; Google Tag Manager; Reca...        NaN   \n",
       "\n",
       "   # of Agents Total  ...  BPO Program Primary Sub-Industry  \\\n",
       "0                NaN  ...          NaN                  NaN   \n",
       "1                NaN  ...          NaN                  NaN   \n",
       "2                NaN  ...          NaN                  NaN   \n",
       "3                NaN  ...          NaN                  NaN   \n",
       "4                NaN  ...          NaN                  NaN   \n",
       "\n",
       "  Number of Sessions  WFM  Country/Region                  Industry  \\\n",
       "0                0.0  NaN   United States  HVAC and plumbing supply   \n",
       "1                NaN  NaN             NaN              Pest Control   \n",
       "2                0.0  NaN   United States              Pest Control   \n",
       "3                0.0  NaN   United States              Pest Control   \n",
       "4                0.0  NaN   United States              Pest Control   \n",
       "\n",
       "        Create Date      Company name  Last Modified Date  BPO Program Tier  \n",
       "0  2024-10-30 10:51  Company_4fc73a2a    2025-02-14 13:39               NaN  \n",
       "1  2024-09-12 18:30  Company_f3f7e884    2025-02-14 13:39               NaN  \n",
       "2  2024-09-03 10:44  Company_89929bed    2025-02-14 13:39               NaN  \n",
       "3  2024-08-02 11:36  Company_33dbf591    2025-02-14 13:39               NaN  \n",
       "4  2024-08-02 11:36  Company_a3079821    2025-02-14 13:39               NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_df = load_csv_from_github(\"anonymized_hubspot_companies.csv\")\n",
    "companies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weighted amount</th>\n",
       "      <th>Deal Description</th>\n",
       "      <th>Cumulative time in \"BANT Deal. Pain ID'ed (Sales Pipeline)\" (HH:mm:ss)</th>\n",
       "      <th>Cumulative time in \"Opportunity (Sales Pipeline)\" (HH:mm:ss)</th>\n",
       "      <th>Days to close</th>\n",
       "      <th>Deal Score</th>\n",
       "      <th>Close Date</th>\n",
       "      <th>Deal source attribution 2</th>\n",
       "      <th>Cumulative time in \"In Trial - Trial in Progress (Sales Pipeline)\" (HH:mm:ss)</th>\n",
       "      <th>Contract Start Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Latest Milestone Update Date</th>\n",
       "      <th>Ticket name</th>\n",
       "      <th>Trial Overview</th>\n",
       "      <th>Trial Start Date</th>\n",
       "      <th>1st Syms presented for review</th>\n",
       "      <th>Project Launch Day</th>\n",
       "      <th>Training: Reporting</th>\n",
       "      <th>Construction of 1st Sym begun</th>\n",
       "      <th>Trial Required</th>\n",
       "      <th>Was the sym QAed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250.0</td>\n",
       "      <td>40-50 employees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2025-05-01 15:26</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3,000 + Agents.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2025-07-20 09:06</td>\n",
       "      <td>Referral Partner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6k agents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2025-07-20 09:03</td>\n",
       "      <td>Referral Partner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Furthest Along - 300 agents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2025-07-20 08:55</td>\n",
       "      <td>Referral Partner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>BPO through partnership with AmplifAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2025-06-20 15:01</td>\n",
       "      <td>Master Agent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weighted amount                       Deal Description  \\\n",
       "0            250.0                        40-50 employees   \n",
       "1              0.0                        3,000 + Agents.   \n",
       "2              0.0                            5-6k agents   \n",
       "3              0.0            Furthest Along - 300 agents   \n",
       "4          10000.0  BPO through partnership with AmplifAI   \n",
       "\n",
       "  Cumulative time in \"BANT Deal. Pain ID'ed (Sales Pipeline)\" (HH:mm:ss)  \\\n",
       "0                                                NaN                       \n",
       "1                                                NaN                       \n",
       "2                                                NaN                       \n",
       "3                                                NaN                       \n",
       "4                                                NaN                       \n",
       "\n",
       "  Cumulative time in \"Opportunity (Sales Pipeline)\" (HH:mm:ss)  Days to close  \\\n",
       "0                                                NaN                     69.0   \n",
       "1                                                NaN                    149.0   \n",
       "2                                                NaN                    149.0   \n",
       "3                                                NaN                    149.0   \n",
       "4                                                NaN                    120.0   \n",
       "\n",
       "   Deal Score        Close Date Deal source attribution 2  \\\n",
       "0        33.0  2025-05-01 15:26                     Event   \n",
       "1        13.0  2025-07-20 09:06          Referral Partner   \n",
       "2        13.0  2025-07-20 09:03          Referral Partner   \n",
       "3        13.0  2025-07-20 08:55          Referral Partner   \n",
       "4        37.0  2025-06-20 15:01              Master Agent   \n",
       "\n",
       "  Cumulative time in \"In Trial - Trial in Progress (Sales Pipeline)\" (HH:mm:ss)  \\\n",
       "0                                                NaN                              \n",
       "1                                                NaN                              \n",
       "2                                                NaN                              \n",
       "3                                                NaN                              \n",
       "4                                                NaN                              \n",
       "\n",
       "  Contract Start Date  ... Latest Milestone Update Date Ticket name  \\\n",
       "0                 NaN  ...                          NaN         NaN   \n",
       "1                 NaN  ...                          NaN         NaN   \n",
       "2                 NaN  ...                          NaN         NaN   \n",
       "3                 NaN  ...                          NaN         NaN   \n",
       "4                 NaN  ...                          NaN         NaN   \n",
       "\n",
       "  Trial Overview Trial Start Date  1st Syms presented for review  \\\n",
       "0            NaN              NaN                            NaN   \n",
       "1            NaN              NaN                            NaN   \n",
       "2            NaN              NaN                            NaN   \n",
       "3            NaN              NaN                            NaN   \n",
       "4            NaN              NaN                            NaN   \n",
       "\n",
       "  Project Launch Day Training: Reporting Construction of 1st Sym begun  \\\n",
       "0                NaN                 NaN                           NaN   \n",
       "1                NaN                 NaN                           NaN   \n",
       "2                NaN                 NaN                           NaN   \n",
       "3                NaN                 NaN                           NaN   \n",
       "4                NaN                 NaN                           NaN   \n",
       "\n",
       "  Trial Required  Was the sym QAed?  \n",
       "0            NaN                NaN  \n",
       "1            NaN                NaN  \n",
       "2            NaN                NaN  \n",
       "3            NaN                NaN  \n",
       "4            NaN                NaN  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in deals merged data for mod\n",
    "df = load_csv_from_github(\"deals_centric_merged.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with less than 90% missing values \n",
    "columns_to_keep = missing_percentage[missing_percentage < 0.7].index\n",
    "\n",
    "# Create a new dataframe with only the columns to keep\n",
    "df_cleaned = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with 30-70% missing values:\n",
      "Cumulative time in \"BANT Deal. Pain ID'ed (Sales Pipeline)\" (HH:mm:ss): 64.76% missing\n",
      "Last Activity Date: 32.21% missing\n",
      "Close Date_company: 54.81% missing\n",
      "ICP Fit Level: 42.66% missing\n",
      "Segmentation: 63.91% missing\n",
      "Type: 32.38% missing\n",
      "Primary Sub-Industry: 45.87% missing\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of missing values in each column\n",
    "missing_percentage = df.isna().mean()\n",
    "\n",
    "# Identify columns with 30-70% missing values\n",
    "columns_30_70_missing = missing_percentage[(missing_percentage >= 0.3) & (missing_percentage <= 0.7)]\n",
    "\n",
    "# Print the columns and their missing percentages\n",
    "print(\"Columns with 30-70% missing values:\")\n",
    "for column, percent in columns_30_70_missing.items():\n",
    "    print(f\"{column}: {percent:.2%} missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imputation pipeline...\n",
      "Identified column types: 8 date columns, 3 time columns, 23 categorical columns, 19 numeric columns\n",
      "Found 7 columns with high missing rates (â‰¥30.0%)\n",
      "Processing date columns...\n",
      "Warning: Could not convert column Consolidated Industry to datetime. Using original values.\n",
      "Processing time columns...\n",
      "Warning: Could not convert any values in Number of times contacted to seconds. Skipping.\n",
      "Warning: Could not convert any values in Time Zone to seconds. Skipping.\n",
      "Processing categorical columns...\n",
      "Processing numeric columns...\n",
      "Warning: 2 columns still have missing values after imputation:\n",
      "  - Time Zone: 36 missing values\n",
      "  - Consolidated Industry: 22 missing values\n",
      "Performing final cleanup...\n",
      "Imputation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laura\\AppData\\Local\\Temp\\ipykernel_13800\\1179323607.py:103: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  result_df[col] = pd.to_datetime(result_df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import re\n",
    "\n",
    "def impute_sales_data(df, high_missing_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Efficient imputation pipeline for CRM sales data, handling both \n",
    "    high-missing-rate columns (30-70%) and low-missing-rate columns (<30%).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe with missing values\n",
    "    high_missing_threshold (float): Threshold to determine which columns need advanced imputation\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with all missing values imputed\n",
    "    \"\"\"\n",
    "    print(\"Starting imputation pipeline...\")\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Check if there are any missing values at all\n",
    "    if not df.isna().any().any():\n",
    "        print(\"No missing values found in dataset. No imputation needed.\")\n",
    "        return result_df\n",
    "    \n",
    "    # Classify columns by data type once for the whole process\n",
    "    date_cols = [col for col in df.columns if 'Date' in col or 'date' in col]\n",
    "    # Expanded time column detection\n",
    "    time_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                ['time', 'HH:mm', 'duration', 'hours', 'minutes'])]\n",
    "    \n",
    "    # Special case for the known cumulative time column\n",
    "    known_time_col = 'Cumulative time in \"BANT Deal. Pain ID\\'ed (Sales Pipeline)\" (HH:mm:ss)'\n",
    "    if known_time_col in df.columns and known_time_col not in time_cols:\n",
    "        time_cols.append(known_time_col)\n",
    "    \n",
    "    # Ensure columns are unique across categories\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in date_cols and col not in time_cols]\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Identify high-missing and low-missing columns\n",
    "    missing_rates = df.isna().mean()\n",
    "    high_missing_cols = missing_rates[missing_rates >= high_missing_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Identified column types: {len(date_cols)} date columns, {len(time_cols)} time columns, \"\n",
    "          f\"{len(categorical_cols)} categorical columns, {len(numeric_cols)} numeric columns\")\n",
    "    print(f\"Found {len(high_missing_cols)} columns with high missing rates (â‰¥{high_missing_threshold*100}%)\")\n",
    "    \n",
    "    # Process each column type with the appropriate method\n",
    "    print(\"Processing date columns...\")\n",
    "    result_df = process_date_columns(result_df, date_cols, high_missing_cols, high_missing_threshold)\n",
    "    \n",
    "    print(\"Processing time columns...\")\n",
    "    result_df = process_time_columns(result_df, time_cols, high_missing_cols)\n",
    "    \n",
    "    print(\"Processing categorical columns...\")\n",
    "    result_df = process_categorical_columns(result_df, categorical_cols, high_missing_cols)\n",
    "    \n",
    "    print(\"Processing numeric columns...\")\n",
    "    result_df = process_numeric_columns(result_df, numeric_cols, high_missing_cols, high_missing_threshold)\n",
    "    \n",
    "    # Verify no NaN values remain\n",
    "    remaining_nans = result_df.isna().sum()\n",
    "    columns_with_nans = remaining_nans[remaining_nans > 0]\n",
    "    \n",
    "    if not columns_with_nans.empty:\n",
    "        print(f\"Warning: {len(columns_with_nans)} columns still have missing values after imputation:\")\n",
    "        for col, count in columns_with_nans.items():\n",
    "            print(f\"  - {col}: {count} missing values\")\n",
    "        print(\"Performing final cleanup...\")\n",
    "        \n",
    "        # Final fallback for any remaining NaN values\n",
    "        for col in columns_with_nans.index:\n",
    "            dtype = result_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(dtype):\n",
    "                result_df[col] = result_df[col].fillna(0)\n",
    "            elif pd.api.types.is_datetime64_dtype(dtype):\n",
    "                result_df[col] = result_df[col].fillna(pd.Timestamp('2020-01-01'))\n",
    "            else:\n",
    "                result_df[col] = result_df[col].fillna('Unknown')\n",
    "    \n",
    "    print(\"Imputation complete!\")\n",
    "    return result_df\n",
    "\n",
    "def process_date_columns(df, date_cols, high_missing_cols, threshold):\n",
    "    \"\"\"Handle date columns based on their missing rate\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in date_cols:\n",
    "        if col not in df.columns or not df[col].isna().any():\n",
    "            continue\n",
    "        \n",
    "        # Create a copy of the original column in case conversion fails\n",
    "        original_values = result_df[col].copy()\n",
    "        \n",
    "        try:\n",
    "            # Convert to datetime\n",
    "            result_df[col] = pd.to_datetime(result_df[col], errors='coerce')\n",
    "            \n",
    "            # Check if conversion was successful\n",
    "            if result_df[col].isna().all():\n",
    "                print(f\"Warning: Could not convert column {col} to datetime. Using original values.\")\n",
    "                result_df[col] = original_values\n",
    "                continue\n",
    "            \n",
    "            if col in high_missing_cols:\n",
    "                # For high missing rates, use a reference date and simple imputation\n",
    "                reference_date = pd.Timestamp('2020-01-01')\n",
    "                \n",
    "                # For datetimes, use median of available dates\n",
    "                valid_dates = result_df[col].dropna()\n",
    "                if not valid_dates.empty:\n",
    "                    median_date = valid_dates.median()\n",
    "                    result_df[col] = result_df[col].fillna(median_date)\n",
    "                else:\n",
    "                    # If all dates are missing, use reference date\n",
    "                    result_df[col] = result_df[col].fillna(reference_date)\n",
    "            else:\n",
    "                # For low missing rates, use forward/backward fill or most frequent\n",
    "                missing_pct = df[col].isna().mean()\n",
    "                if missing_pct > 0.05:  # More than 5% missing\n",
    "                    # First try to sort by a related date column if possible\n",
    "                    date_cols_sorted = sorted(date_cols, key=lambda x: df[x].isna().mean())\n",
    "                    for sort_col in date_cols_sorted:\n",
    "                        if sort_col != col and not df[sort_col].isna().all() and pd.api.types.is_datetime64_dtype(result_df[sort_col]):\n",
    "                            try:\n",
    "                                # Try sorting by this column to improve ffill/bfill\n",
    "                                temp_df = result_df[[sort_col, col]].sort_values(sort_col)\n",
    "                                temp_df[col] = temp_df[col].ffill().bfill()\n",
    "                                result_df[col] = temp_df[col]\n",
    "                                break\n",
    "                            except:\n",
    "                                continue\n",
    "                    \n",
    "                    # If no sorting worked, just fill without sorting\n",
    "                    if result_df[col].isna().any():\n",
    "                        result_df[col] = result_df[col].ffill().bfill()\n",
    "                else:\n",
    "                    # For very low missing rates, use most frequent date\n",
    "                    most_freq_date = result_df[col].mode()[0]\n",
    "                    result_df[col] = result_df[col].fillna(most_freq_date)\n",
    "            \n",
    "            # Convert back to original string format if needed\n",
    "            if df[col].dtype == 'object':\n",
    "                try:\n",
    "                    has_time = original_values.astype(str).str.contains(':').any()\n",
    "                    format_str = '%m/%d/%Y %H:%M' if has_time else '%m/%d/%Y'\n",
    "                    result_df[col] = result_df[col].dt.strftime(format_str)\n",
    "                except:\n",
    "                    # If datetime formatting fails, keep as is\n",
    "                    print(f\"Warning: Could not format dates in {col} back to string. Keeping datetime format.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing date column {col}: {e}\")\n",
    "            # Restore original values if processing failed\n",
    "            result_df[col] = original_values\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def process_time_columns(df, time_cols, high_missing_cols):\n",
    "    \"\"\"Handle time columns (HH:MM:SS format)\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if col not in df.columns or not df[col].isna().any():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Keep original values in case of error\n",
    "            original_values = result_df[col].copy()\n",
    "            \n",
    "            # Convert time to seconds for processing, with error handling\n",
    "            try:\n",
    "                result_df[f'{col}_seconds'] = df[col].apply(\n",
    "                    lambda x: sum(int(t) * m for t, m in zip(x.split(':'), [3600, 60, 1])) \n",
    "                    if isinstance(x, str) and re.match(r'\\d+:\\d+:\\d+', x) else np.nan\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error converting time column {col} to seconds: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if conversion was successful\n",
    "            if result_df[f'{col}_seconds'].isna().all():\n",
    "                print(f\"Warning: Could not convert any values in {col} to seconds. Skipping.\")\n",
    "                # Clean up and continue to next column\n",
    "                if f'{col}_seconds' in result_df.columns:\n",
    "                    result_df = result_df.drop(columns=[f'{col}_seconds'])\n",
    "                continue\n",
    "            \n",
    "            # For any column, use median imputation (simple and effective for time)\n",
    "            median_seconds = result_df[f'{col}_seconds'].median()\n",
    "            if pd.isna(median_seconds):  # If median is NaN, use 0\n",
    "                median_seconds = 0\n",
    "            \n",
    "            result_df[f'{col}_seconds'] = result_df[f'{col}_seconds'].fillna(median_seconds)\n",
    "            \n",
    "            # Convert seconds back to HH:MM:SS format\n",
    "            try:\n",
    "                result_df[col] = result_df[f'{col}_seconds'].apply(\n",
    "                    lambda x: f\"{int(x // 3600):d}:{int((x % 3600) // 60):02d}:{int(x % 60):02d}\" \n",
    "                    if not pd.isna(x) else \"\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error converting seconds back to time format for {col}: {e}\")\n",
    "                result_df[col] = original_values\n",
    "            \n",
    "            # Drop the temporary seconds column\n",
    "            if f'{col}_seconds' in result_df.columns:\n",
    "                result_df = result_df.drop(columns=[f'{col}_seconds'])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing time column {col}: {e}\")\n",
    "            # Make sure column still exists in result_df\n",
    "            if col not in result_df.columns:\n",
    "                result_df[col] = original_values\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def process_categorical_columns(df, categorical_cols, high_missing_cols):\n",
    "    \"\"\"Handle categorical columns\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Handle industry columns as a special case for consistency\n",
    "    industry_cols = [col for col in categorical_cols if 'Industry' in col]\n",
    "    if len(industry_cols) > 1:\n",
    "        # Try to fill missing values from other industry columns first\n",
    "        for i, col1 in enumerate(industry_cols):\n",
    "            for col2 in industry_cols[i+1:]:\n",
    "                # Where col1 is null but col2 isn't, copy from col2\n",
    "                mask = df[col1].isna() & df[col2].notna()\n",
    "                result_df.loc[mask, col1] = df.loc[mask, col2]\n",
    "                \n",
    "                # And vice versa\n",
    "                mask = df[col2].isna() & df[col1].notna()\n",
    "                result_df.loc[mask, col2] = df.loc[mask, col1]\n",
    "    \n",
    "    # Now process remaining categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col not in df.columns or not df[col].isna().any():\n",
    "            continue\n",
    "            \n",
    "        if col in high_missing_cols:\n",
    "            # For high missing rates, determine approach based on cardinality\n",
    "            unique_count = df[col].nunique()\n",
    "            \n",
    "            if unique_count > 10:\n",
    "                # Many categories - mark as \"Unknown\"\n",
    "                result_df[col] = result_df[col].fillna('Unknown')\n",
    "            else:\n",
    "                # Few categories - use most frequent value\n",
    "                most_freq = df[col].mode()[0]\n",
    "                result_df[col] = result_df[col].fillna(most_freq)\n",
    "        else:\n",
    "            # For low missing rates, use most frequent\n",
    "            most_freq = df[col].mode()[0]\n",
    "            result_df[col] = result_df[col].fillna(most_freq)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def process_numeric_columns(df, numeric_cols, high_missing_cols, threshold):\n",
    "    \"\"\"Handle numeric columns\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Split financial columns vs other numeric\n",
    "    financial_cols = [col for col in numeric_cols if any(term in col.lower() for term in \n",
    "                    ['amount', 'revenue', 'forecast', 'weighted'])]\n",
    "    other_numeric = [col for col in numeric_cols if col not in financial_cols]\n",
    "    \n",
    "    # Process financial columns\n",
    "    for col in financial_cols:\n",
    "        if col not in df.columns or not df[col].isna().any():\n",
    "            continue\n",
    "            \n",
    "        # Use median for financial columns regardless of missing rate\n",
    "        # (simpler and often effective for financial data)\n",
    "        median_val = df[col].median()\n",
    "        result_df[col] = result_df[col].fillna(median_val)\n",
    "    \n",
    "    # Group remaining numeric columns by missing rate\n",
    "    high_missing_numeric = [col for col in other_numeric if col in high_missing_cols]\n",
    "    low_missing_numeric = [col for col in other_numeric if col not in high_missing_cols]\n",
    "    \n",
    "    # For high missing numeric, use MICE if there are multiple columns, otherwise use median\n",
    "    if high_missing_numeric:\n",
    "        # Filter to columns that actually have missing values\n",
    "        cols_with_missing = [col for col in high_missing_numeric if df[col].isna().any()]\n",
    "        \n",
    "        if len(cols_with_missing) > 1:  # Multiple columns - use MICE\n",
    "            imputer = IterativeImputer(\n",
    "                estimator=RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42),\n",
    "                max_iter=5,  # Reduced iterations\n",
    "                random_state=42,\n",
    "                skip_complete=True\n",
    "            )\n",
    "            \n",
    "            # Get only rows where at least one column has a value (not all missing)\n",
    "            valid_rows = df[cols_with_missing].notna().any(axis=1)\n",
    "            \n",
    "            if valid_rows.any():\n",
    "                # Create temporary dataframe with only columns to impute\n",
    "                temp_df = df.loc[valid_rows, cols_with_missing].copy()\n",
    "                \n",
    "                # Apply imputation\n",
    "                imputed_values = imputer.fit_transform(temp_df)\n",
    "                \n",
    "                # Update result dataframe for these columns and rows\n",
    "                for i, col in enumerate(cols_with_missing):\n",
    "                    result_df.loc[valid_rows, col] = imputed_values[:, i]\n",
    "                \n",
    "                # For remaining rows, use median\n",
    "                for col in cols_with_missing:\n",
    "                    result_df[col] = result_df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                # If all rows are completely missing, use median\n",
    "                for col in cols_with_missing:\n",
    "                    result_df[col] = result_df[col].fillna(0)  # Default to 0\n",
    "        else:\n",
    "            # Single column - use median (faster and simpler)\n",
    "            for col in cols_with_missing:\n",
    "                result_df[col] = result_df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For low missing numeric, use simple median imputation\n",
    "    if low_missing_numeric:\n",
    "        cols_with_missing = [col for col in low_missing_numeric if df[col].isna().any()]\n",
    "        \n",
    "        for col in cols_with_missing:\n",
    "            result_df[col] = result_df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Special case: web analytics columns should be 0 if missing\n",
    "    web_cols = [col for col in numeric_cols if any(term in col for term in \n",
    "                ['Pageviews', 'Sessions', 'Submissions', 'contacted'])]\n",
    "    \n",
    "    for col in web_cols:\n",
    "        if col in result_df.columns and result_df[col].isna().any():\n",
    "            result_df[col] = result_df[col].fillna(0)\n",
    "    \n",
    "    return result_df\n",
    "df_imputed = impute_sales_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable Distribution:\n",
      "Is Closed Won\n",
      "False    0.733558\n",
      "True     0.266442\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Gradient Boosting with SMOTE ===\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "{0: 348, 1: 348}\n",
      "\n",
      "Cross-validation F1 Scores with SMOTE:\n",
      "[0.86713287 0.87142857 0.9        0.8951049  0.82706767]\n",
      "Mean CV F1 Score: 0.8721 (+/- 0.0519)\n",
      "\n",
      "SMOTE Model Performance:\n",
      "Confusion Matrix:\n",
      "[[77 10]\n",
      " [ 6 26]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91        87\n",
      "           1       0.72      0.81      0.76        32\n",
      "\n",
      "    accuracy                           0.87       119\n",
      "   macro avg       0.82      0.85      0.84       119\n",
      "weighted avg       0.87      0.87      0.87       119\n",
      "\n",
      "\n",
      "Additional Metrics:\n",
      "ROC AUC Score: 0.9088\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                                        feature  importance\n",
      "1                           Number of Pageviews    0.226414\n",
      "6                                        Amount    0.072830\n",
      "5                               Forecast amount    0.069822\n",
      "7                    Amount in company currency    0.056186\n",
      "157                           Deal Type_Renewal    0.054112\n",
      "145  Deal source attribution 2_SymTrain EE name    0.052874\n",
      "156                                Deal Type_PS    0.047228\n",
      "4                     Number of times contacted    0.045344\n",
      "3                            Number of Sessions    0.039995\n",
      "0                                 Days to close    0.037739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SMOTE for handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def prepare_features(df):\n",
    "    # Columns to exclude - including potentially leaky features\n",
    "    exclude_columns = [\n",
    "        'Is Closed Won',  # Target variable\n",
    "        'Is Closed (numeric)',\n",
    "        'Is closed lost',\n",
    "        'Is Deal Closed?',\n",
    "        'Record ID',\n",
    "        'Record ID_company',\n",
    "        'Close Date',\n",
    "        'Last Modified Date',\n",
    "        'Create Date',\n",
    "        'Deal Name',\n",
    "        'Company name',\n",
    "        'Deal probability',  # Potential leakage\n",
    "        'Weighted amount',  # Derived from probability\n",
    "        'Weighted amount in company currency'  # Also likely leaky\n",
    "    ]\n",
    "\n",
    "    # Features for consideration\n",
    "    candidate_features = [\n",
    "        # Business Context\n",
    "        'Days to close',\n",
    "        'Number of Pageviews',\n",
    "        'Number of Form Submissions',\n",
    "        'Number of Sessions',\n",
    "        'Number of times contacted',\n",
    "        \n",
    "        # Financial Indicators\n",
    "        'Forecast amount',\n",
    "        'Amount',\n",
    "        'Amount in company currency',\n",
    "        'Annual Revenue',\n",
    "        \n",
    "        # Company Attributes\n",
    "        'ICP Fit Level',\n",
    "        'Industry',\n",
    "        'Primary Industry',\n",
    "        'Number of Employees',\n",
    "        'Year Founded',\n",
    "        \n",
    "        # Engagement Metrics\n",
    "        'Deal source attribution 2',\n",
    "        'Original Traffic Source',\n",
    "        'Pipeline',\n",
    "        'Deal Type',\n",
    "        'Segmentation'\n",
    "    ]\n",
    "\n",
    "    # Filter and prepare features\n",
    "    features = [col for col in candidate_features if col in df.columns]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[features]\n",
    "    y = df['Is Closed Won']\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols)\n",
    "\n",
    "    # Ensure target is numeric\n",
    "    if y.dtype == 'object' or y.dtype == 'bool':\n",
    "        y = y.map({True: 1, 'True': 1, False: 0, 'False': 0})\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Print target distribution\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(df_imputed['Is Closed Won'].value_counts(normalize=True))\n",
    "\n",
    "# Prepare the data\n",
    "X, y = prepare_features(df_imputed)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 1. Baseline Gradient Boosting with SMOTE\n",
    "print(\"\\n=== Gradient Boosting with SMOTE ===\")\n",
    "# Create pipeline with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print class distribution after SMOTE\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "unique, counts = np.unique(y_train_resampled, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Gradient Boosting with SMOTE\n",
    "gb_smote = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_smote = cross_val_score(\n",
    "    gb_smote, \n",
    "    X_train_resampled, \n",
    "    y_train_resampled, \n",
    "    cv=cv, \n",
    "    scoring='f1'\n",
    ")\n",
    "print(\"\\nCross-validation F1 Scores with SMOTE:\")\n",
    "print(cv_scores_smote)\n",
    "print(f\"Mean CV F1 Score: {cv_scores_smote.mean():.4f} (+/- {cv_scores_smote.std() * 2:.4f})\")\n",
    "\n",
    "# Fit and predict with SMOTE\n",
    "gb_smote.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_smote = gb_smote.predict(X_test)\n",
    "y_pred_proba_smote = gb_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nSMOTE Model Performance:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_smote))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "\n",
    "print(\"\\nAdditional Metrics:\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba_smote):.4f}\")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': gb_smote.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
